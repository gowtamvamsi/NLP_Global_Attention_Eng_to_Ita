{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. Download the Italian to English translation dataset from <a href=\"http://www.manythings.org/anki/ita-eng.zip\">here</a>\n",
    "2. You will find ita.txt file in that ZIP, you can read that data using python and preprocess that data. \n",
    "3. You have to implement an Encoder and Decoder architecture with Luong attention.\n",
    "\n",
    "Encoder - 1 layer LSTM \n",
    "Decoder - 1 layer LSTM \n",
    "attention - Luone attention. \n",
    "\n",
    "You can read Luonge attention from This <a href=\"https://arxiv.org/pdf/1508.04025.pdf\">this</a> paper. <a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\">this</a> is one of the best resource you can find in google, please go through it.\n",
    "\n",
    "You can check some high level overview in below images. You have to use only Global attention. In Global attention, we have 3 types of scoring functions. please create one model for each scoring function. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<pre><font size=5><b>Luonge Attention (Multiplicative Attention)</b></font>\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/luong2015-fig2-3.png\">\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*4y96boGNMiNVHNo8.\">\n",
    "<img src=\"https://i.stack.imgur.com/RaTOU.png\"></pre>\n",
    "\n",
    "\n",
    "\n",
    "4. Using attention weights, you can plot the attention plots, please plot those for 2-3 examples. You can check about those in <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate\">this</a>\n",
    "\n",
    "5. The attention layer has to be written by yourself only. The main objective of this assignment is to read and implement a paper on yourself so please do it yourself.  \n",
    "\n",
    "6. You can use any tf.Keras highlevel API's to build and train the models. \n",
    "\n",
    "7. Use BLEU score as metric to evaluate your model. You can use any loss function you need.\n",
    "\n",
    "8. You have to use Tensorboard to plot the Graph, Scores and histograms of gradients. \n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Writing a custom layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before we write custom layers in tensorflow lets see the definition of <b>Layers</b> class\n",
    "\n",
    "<a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer'> tf.keras.layers.Layers</a>\n",
    "\n",
    "From the tf documentation\n",
    "<pre>\n",
    "This is the class from which all layers inherit.\n",
    "\n",
    "A layer is a class implementing common neural networks operations, such as convolution, batch norm, etc. These operations require managing weights, losses, updates, and inter-layer connectivity.\n",
    "\n",
    "Users will just instantiate a layer and then treat it as a callable.\n",
    "\n",
    "We recommend that descendants of Layer implement the following methods:\n",
    "\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|                                                                                                                   |\n",
    "|<strong> <font color='green'>def __init__(self, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):</font>                               |\n",
    "+</strong>-------------------------------------------------------------------------------------------------------------------+                                                                                                                 \n",
    "|                                                                                                                   |\n",
    "|* the properties should be set by the user via keyword arguments.                                                  |\n",
    "|                                                                                                                   |\n",
    "|* note that 'dtype', 'input_shape' and 'batch_input_shape' are only applicable to input layers, do not pass these  |\n",
    "|  keywords to non-input layers.                                                                                    |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|* allowed_kwargs = {'input_shape', 'batch_input_shape', 'batch_size', 'weights', 'activity_regularizer','autocast'}|\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "\n",
    "\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|<strong> <font color='green'>def build(self, input_shape)</font></strong>:                                                                                     |                                                                                       +-------------------------------------------------------------------------------------------------------------------+\n",
    "|                                                                                                                   |\n",
    "| * Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of |\n",
    "|   subclasses of `Layer` or `Model`                                                                                |\n",
    "|                                                                                                                   |\n",
    "| * You can override if you need a state-creation step in-between <em><font color='blue'>layer instantiation</font></em> and <em><font color='blue'>layer call</font></em>.               |\n",
    "|                                                                                                                   |\n",
    "| * This is typically used to create the weights of `Layer` subclasses.                                             |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "| Arguments:                                                                                                        |\n",
    "|    input_shape:                                                                                                   |\n",
    "|    Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs         |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "| <strong> <font color='green'>def call(self, inputs, **kwargs)</font></strong>:                                                                                |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "| * This is where the layer's logic lives.                                                                          |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|* Arguments:                                                                                                       |\n",
    "|        inputs: Input tensor, or list/tuple of input tensors.                                                      |\n",
    "|        **kwargs: Additional keyword arguments.                                                                    |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|* Returns:                                                                                                         |\n",
    "|        A tensor or list/tuple of tensors.                                                                         |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "    \n",
    "<a href='https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/engine/base_layer.py#L310'>check for more arguments</a>                               \n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|<strong> <font color='green'>def add_weight(self,name=None, shape=None, ..., **kwargs)</font></strong>:                                                        |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|* Adds a new variable to the layer.                                                                                |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|* Arguments:                                                                                                       |\n",
    "|        name : Variable name.                                                                                      |\n",
    "|        shape: Variable shape. Defaults to scalar if unspecified.                                                  |\n",
    "|        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.                                    |\n",
    "|        ...                                                                                                        |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "|* Returns:                                                                                                         |\n",
    "|        The created variable. Usually either a `Variable` or `ResourceVariable` instance.                          |\n",
    "+-------------------------------------------------------------------------------------------------------------------+\n",
    "...\n",
    "there are other functions also availabel, please check this link for better understanding of it\n",
    "<a href='https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/engine/base_layer.py'>base_layer.py</a>\n",
    "\n",
    "</pre>\n",
    "\n",
    "## 1.1 Example\n",
    "super(): https://stackoverflow.com/a/27134600/4084039\n",
    "<img src='https://i.imgur.com/1a8N7gH.png' width=600>\n",
    "\n",
    "## 1.2 Resources\n",
    "Do read this blog for more information: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "few screenshots from the above blog\n",
    "\n",
    "1.\n",
    "<img src='https://i.imgur.com/SDNQgos.png' width=600>\n",
    "2.\n",
    "<img src='https://i.imgur.com/syqjpux.png' width=600>\n",
    "3. \n",
    "<img src='https://i.imgur.com/PfmYWno.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Writing a custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ways to implement a model architecture in TF\n",
    "<img src='https://i.imgur.com/n7DBcoo.png' width=400>\n",
    "The third and final method to implement a model architecture using Keras and TensorFlow 2.0 is called model subclassing.\n",
    "\n",
    "Inside of tf.keras the `Model` class is the root class used to define a model architecture. Since tf.keras utilizes object-oriented programming, we can actually `subclass` the Model class and then insert our architecture definition.\n",
    "\n",
    "<pre>\n",
    "    The `Model` class has the same API as `Layer`, with the following differences:\n",
    "        It exposes built-in training, evaluation, and prediction loops (model.fit(), model.evaluate(), model.predict()).\n",
    "        It exposes the list of its inner layers, via the `model.layers` property.\n",
    "        It exposes saving and serialization APIs.\n",
    "    \n",
    "    <font color='blue'>Effectively, the \"Layer\" class corresponds to what we refer to in the literature as a \"layer\" (as in \"convolution layer\" or \"recurrent layer\") or as a \"block\" (as in \"ResNet block\" or \"Inception block\").\n",
    "\n",
    "    Meanwhile, the \"Model\" class corresponds to what is referred to in the literature as a \"model\" (as in \"deep learning model\") or as a \"network\" (as in \"deep neural network\").\n",
    "    </font>\n",
    "</pre>\n",
    "## 2. 1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-10bee31156ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyDenseLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#https://stackoverflow.com/a/27134600/4084039\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs, **kwargs):\n",
    "        super().__init__(**kwargs) #https://stackoverflow.com/a/27134600/4084039\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\n",
    "        \n",
    "    def call(self, input):\n",
    "        print(input.shape,self.kernel.shape)\n",
    "        return tf.matmul(input, self.kernel)\n",
    "\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self, num_inputs, num_outputs, rnn_units):\n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        self.dense = MyDenseLayer(num_outputs, name='myDenseLayer')\n",
    "#         self.lstmcell = tf.keras.layers.LSTMCell(rnn_units)\n",
    "#         self.rnn = RNN(self.lstmcell)\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "    def call(self, input):\n",
    "#         output = self.rnn(input)\n",
    "        output = self.dense(input)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "import numpy as np\n",
    "data = np.zeros([10, 5])\n",
    "y = np.zeros([10,2])\n",
    "\n",
    "model  = MyModel(num_inputs=5, num_outputs=2, rnn_units=32)\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer,loss=loss_object)\n",
    "model.fit(data,y, steps_per_epoch=1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encode decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "First, let's talk about why we need Attention models and before that lets review how basic seq2seq models work. Normal seq2seq models first process all the parts in the Input sequence and generate a context vector. This context vector is then forwarded to the decoder and then it will start producing the Output sequence. This architecture works fine with small input sequences but not with long sequences the reason being normal seq2seq model is not being able to preserve dependencies of words at the start with Context vector. Refer the below image to visualize the working of normal Seq2Seq Model. \n",
    "<img src=\"./Attention_1.jpeg\" style=\"width: 600px;\">\n",
    "\n",
    "Performace of a simple Seq2Seq Model:\n",
    "\n",
    "<img src=\"./Attention_2.jpeg\" style=\"width: 600px;\">\n",
    "\n",
    "Attention models will solve the long term dependency problem by performing the task the way humans do the translation. For example, let's consider we have to translate long English sentence to Hindi. The way we do this is by reading the first few words then translate it and go on to the next few words. Most of the times we perform our translation on giving importance to one word over others. This is exactly how attention models work.\n",
    "In a normal Seq2Seq model, it will only use the Context vector generated at the end of the Encoder discarding rest of the hidden states but Attention model will use the hidden states.\n",
    "\n",
    "Architecture of Attention Models:\n",
    "\n",
    "<img src=\"./Attention_3.jpeg\" style=\"width: 600px;\">\n",
    "\n",
    "In the above image c1, c2, c3.. these are the context vectors which will be inputted to Decoder in each state. Each context vector is created by taking the sum of hi*alpha_i ( i=1 to 3 in the above case). We will create alpha in such a manner such sum of all the alpha's contributing to one context vector will be 1. It makes logical sense because by that constraint we can teach the model to give the highest alpha value to the word which is most relevant in that time step.\n",
    "<img src=\"./Attention_4.jpeg\" style=\"width: 400px;\">\n",
    "\n",
    "\n",
    "There are two types of Attention concepts:\n",
    "1) Local Attention\n",
    "2) Global Attention\n",
    "\n",
    "Attention architecture explained above is the base for local attention but in this article, we will use Global attention. Global attention means we will consider all the hidden states when we are calculating the context vector for 1 word (Tx in the above image will be length(input_sequence)).\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.\\tCiao!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #607364 (Cero)',\n",
       " 'Run!\\tCorri!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906347 (Guybrush88)',\n",
       " 'Run!\\tCorra!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906348 (Guybrush88)',\n",
       " 'Run!\\tCorrete!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906350 (Guybrush88)',\n",
       " 'Who?\\tChi?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #2126402 (Guybrush88)',\n",
       " 'Wow!\\tWow!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #1922050 (Guybrush88)',\n",
       " 'Jump!\\tSalta!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1102981 (jamessilver) & #1543215 (Guybrush88)',\n",
       " 'Jump!\\tSalti!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1102981 (jamessilver) & #4356755 (Guybrush88)',\n",
       " 'Jump!\\tSaltate!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1102981 (jamessilver) & #4356756 (Guybrush88)',\n",
       " 'Jump.\\tSalta.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416939 (Guybrush88)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open('./ita.txt', encoding='UTF-8').read().strip().split('\\n')\n",
    "lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335031"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "exclude = set(string.punctuation)\n",
    "# remove numbers\n",
    "remove_digits = str.maketrans('', '', string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eng_sentence(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(\"'\", '', sent)\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.translate(remove_digits)\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent)\n",
    "    sent = '<start> ' + sent + ' <end>'\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ita_sentence(sent):\n",
    "    sent = re.sub(\"'\", '', sent)\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent) \n",
    "    sent = '<start> ' + sent + ' <end>'\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2941fd993b334c57b2eaf4cf1bf7cc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=335031), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['<start> i want that <end>', '<start> La voglio <end>'],\n",
       " ['<start> i want that <end>', '<start> Lo voglio <end>'],\n",
       " ['<start> i want that <end>', '<start> Io lo voglio <end>'],\n",
       " ['<start> i want that <end>', '<start> Io la voglio <end>'],\n",
       " ['<start> i want them <end>', '<start> Voglio loro <end>'],\n",
       " ['<start> i want them <end>', '<start> Io voglio loro <end>'],\n",
       " ['<start> i want them <end>', '<start> Li voglio <end>'],\n",
       " ['<start> i want them <end>', '<start> Io li voglio <end>'],\n",
       " ['<start> i want them <end>', '<start> Le voglio <end>'],\n",
       " ['<start> i want them <end>', '<start> Io le voglio <end>']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "sent_pairs = []\n",
    "for line in tqdm(lines):\n",
    "    sent_pair = []\n",
    "    #print(line.split('\\t'))\n",
    "    eng, ita = line.split('\\t')[0:2]\n",
    "    eng = preprocess_eng_sentence(eng)\n",
    "    sent_pair.append(eng)\n",
    "    ita = preprocess_ita_sentence(ita)\n",
    "    sent_pair.append(ita)\n",
    "    sent_pairs.append(sent_pair)\n",
    "sent_pairs[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing words to numbers and vice versa\n",
    "class LanguageIndexing():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding train and text data\n",
    "def load_dataset(pairs, num_examples):  \n",
    "    inp_lang = LanguageIndexing(en for en, ma in pairs)\n",
    "    targ_lang = LanguageIndexing(ma for en, ma in pairs)\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301527, 301527, 33504, 33504)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_test, target_tensor_train, target_tensor_test = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state = 101)\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_test), len(target_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "# If you have more GPU idle space you can increase the batch size\n",
    "BATCH_SIZE = 16\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "# If your GPU utilization is low increase the num of units\n",
    "units = 512\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using CuDNNLSTM because it is the fastest implementation of LSTM using GPU\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state,_ = self.lstm(x)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "I mentioned above that we calculate alpha's by using some function on c_i and decoder_output from previous state Loung Attention provides 3 such functions to calculate alpha's.\n",
    "<img src=\"https://i.stack.imgur.com/RaTOU.png\">\n",
    "In this article we are using general scoring method.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(self.dec_units, return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # genral Scoring method.\n",
    "        new_scoring = tf.einsum('bnm,bkm->bnk', enc_output, self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # We use this to make the sum of all attention weigts to 1 \n",
    "        attention_weights = tf.nn.softmax(new_scoring, axis=1)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Concating decoder_output of previous time stamp and context vector.\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state,_= self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # We are using dense layer to get the probabilities of output word w.r.t to train Vocabulary\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "EPOCHS = 8\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            #print(inp.shape)\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            #print(enc_output.shape)\n",
    "            #print(enc_hidden.shape)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                #print(dec_input)\n",
    "                #print('dkjbvwobwenv')\n",
    "                #g=0\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # Making current decoder output as input to decoder in next step.\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        #break\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch%500==0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    #break\n",
    "    # saving (checkpoint) the model every epoch\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x13d36c048>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "For each sentence we translate from English to Italian we will plot attention weights of each Italian word vs all the English words. By watching the attention weights we can visualize how well the model is performing.\n",
    "\n",
    "We are using the BLEU score as a metric to test our model. BLEU is a modification of precision to correct the wild inaccuracy of precision in machine translation tasks.\n",
    "\n",
    "Lets take the below example:\n",
    "<img src=\"./Attention_5.png\" style=\"width: 400px;\">\n",
    "\n",
    "1) In the above image Candidate is machine translated sentence, references are input sentences. All the seven words in the Candidate all are there in the references. Hence, precision (p) = 7/7 = 1\n",
    "2) This problem is rectified in BLEU. In BLEU for each word in the candidate translation, the algorithm takes its maximum total count, m_max, in any of the reference translations. In the example above, the word \"the\" appears twice in reference 1, and once in reference 2. Thus m_max = 2. \n",
    "3) BLEU (candidate,references) = 2/7\n",
    "4) The above-mentioned definition is Vanilla BLEU score we use nltk.translate.bleu which uses n-grams Comparision i =n references and candidate and if there is no n-grams overlap for any order of n-grams, BLEU returns the value 0.\n",
    "5) To avoid this harsh behaviour when no n-gram overlaps are found we used smoothing function.\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    \n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = ''\n",
    "    for i in inputs[0]:\n",
    "        if i == 0:\n",
    "            break\n",
    "        sentence = sentence + inp_lang.idx2word[i] + ' '\n",
    "    sentence = sentence[:-1]\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    # Passing the whole input sequence to Encoder and getting all the hidden states\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "    print(dec_input.shape)\n",
    "    print(dec_hidden.shape)\n",
    "    print(enc_out.shape)\n",
    "    # Predicting 1 word per iteration.\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to visualize how much each output is dependent on every input word.\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate import bleu\n",
    "import random\n",
    "def predict_random_val_sentence(num,is_viz):\n",
    "    if num > len(input_tensor_test):\n",
    "        print('test length exceeded')\n",
    "        return\n",
    "    bleu_score=0\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    for i in tqdm(range(num)):\n",
    "        random_input = input_tensor_test[i]\n",
    "        random_output = target_tensor_test[i]\n",
    "        random_input = np.expand_dims(random_input,0)\n",
    "        result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        actual_sent = ''\n",
    "        for i in random_output:\n",
    "            if i == 0:\n",
    "                break\n",
    "            actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
    "        actual_sent = actual_sent[8:-7]\n",
    "        hypothesis =  result[:-6].split(\" \")\n",
    "        reference = actual_sent.split(\" \")\n",
    "        bleu_score += bleu([reference], hypothesis,smoothing_function=smoothie)\n",
    "        if is_viz==True:\n",
    "            # 8,-6 because or <start> and <end> tags\n",
    "            print('Input sentence: {}'.format(sentence[8:-6]))\n",
    "            print('Translated sentnce: {}'.format(result[:-6]))\n",
    "            print('Actual translation: {}'.format(actual_sent))\n",
    "            attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n",
    "            sentence, result = sentence.split(' '), result.split(' ')\n",
    "            sentence = sentence[1:-1]\n",
    "            result = result[:-2]\n",
    "\n",
    "            trace = go.Heatmap(z = attention_plot, x = sentence, y = result, colorscale='Reds')\n",
    "            data=[trace]\n",
    "            iplot(data)\n",
    "    \n",
    "    print(\"Bleu score on the text corpus is \"+str(bleu_score/num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfbd8caebed49d5aa4f1663e4072eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 512)\n",
      "(1, 49, 512)\n",
      "Input sentence: were not guilty\n",
      "Translated sentnce: figlia pregevoli Alaska Liberò Tiratevi straccio rilassarvi correggeteli stappate tradizionali arata sprofondò Canzonai Compreremo Pitturavo astiene spiona guardato deludeteci lasciami allievi inventario posato Formarono suo guardino preparerò didentità fornirebbe Moltissime collera risparmiò culmine risposerò dimentichiamocelo fotografie freccetta Rubare perfettina precauzione Avenue allentò imparerà piovuto Blanco assicurate enciclopedia caviale Ce comprarono ot\n",
      "Actual translation: Non siamo colpevoli\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Reds",
         "type": "heatmap",
         "uid": "2ede232b-7de2-4525-a92f-f3f09465af7a",
         "x": [
          "were",
          "not",
          "guilty"
         ],
         "y": [
          "figlia",
          "pregevoli",
          "Alaska",
          "Liberò",
          "Tiratevi",
          "straccio",
          "rilassarvi",
          "correggeteli",
          "stappate",
          "tradizionali",
          "arata",
          "sprofondò",
          "Canzonai",
          "Compreremo",
          "Pitturavo",
          "astiene",
          "spiona",
          "guardato",
          "deludeteci",
          "lasciami",
          "allievi",
          "inventario",
          "posato",
          "Formarono",
          "suo",
          "guardino",
          "preparerò",
          "didentità",
          "fornirebbe",
          "Moltissime",
          "collera",
          "risparmiò",
          "culmine",
          "risposerò",
          "dimentichiamocelo",
          "fotografie",
          "freccetta",
          "Rubare",
          "perfettina",
          "precauzione",
          "Avenue",
          "allentò",
          "imparerà",
          "piovuto",
          "Blanco",
          "assicurate",
          "enciclopedia",
          "caviale",
          "Ce",
          "comprarono"
         ],
         "z": [
          [
           0.020395765081048012,
           0.020399510860443115,
           0.020400796085596085
          ],
          [
           0.020403439179062843,
           0.02040637843310833,
           0.020407628268003464
          ],
          [
           0.020397108048200607,
           0.020402805879712105,
           0.02040446177124977
          ],
          [
           0.02042507566511631,
           0.020426997914910316,
           0.02042900212109089
          ],
          [
           0.020408837124705315,
           0.020413434132933617,
           0.020413484424352646
          ],
          [
           0.02041882462799549,
           0.020419759675860405,
           0.020423099398612976
          ],
          [
           0.020396852865815163,
           0.020398659631609917,
           0.020406097173690796
          ],
          [
           0.020414678379893303,
           0.02041710540652275,
           0.020418493077158928
          ],
          [
           0.02040308341383934,
           0.02040311135351658,
           0.02040119096636772
          ],
          [
           0.02040223963558674,
           0.020406009629368782,
           0.02040562964975834
          ],
          [
           0.020406804978847504,
           0.02041100338101387,
           0.020411742851138115
          ],
          [
           0.020425690338015556,
           0.02042669616639614,
           0.02042372338473797
          ],
          [
           0.020433271303772926,
           0.02043539099395275,
           0.020433610305190086
          ],
          [
           0.02041829563677311,
           0.0204179510474205,
           0.020420249551534653
          ],
          [
           0.020417682826519012,
           0.02041599713265896,
           0.02042311243712902
          ],
          [
           0.020406324416399002,
           0.02040700428187847,
           0.02040827088057995
          ],
          [
           0.020421138033270836,
           0.020424284040927887,
           0.020424045622348785
          ],
          [
           0.02042362280189991,
           0.020426683127880096,
           0.020425401628017426
          ],
          [
           0.020409535616636276,
           0.020413266494870186,
           0.02041909657418728
          ],
          [
           0.020412810146808624,
           0.02040928602218628,
           0.020409876480698586
          ],
          [
           0.020410796627402306,
           0.020411141216754913,
           0.02041318081319332
          ],
          [
           0.02040826715528965,
           0.02040942572057247,
           0.02041500061750412
          ],
          [
           0.0204131118953228,
           0.020411044359207153,
           0.02041759341955185
          ],
          [
           0.020409949123859406,
           0.020410429686307907,
           0.02041080966591835
          ],
          [
           0.020426271483302116,
           0.02042290009558201,
           0.020418046042323112
          ],
          [
           0.020413096994161606,
           0.02041323110461235,
           0.020413847640156746
          ],
          [
           0.020411981269717216,
           0.02040986716747284,
           0.020411545410752296
          ],
          [
           0.02040516585111618,
           0.020407147705554962,
           0.020411627367138863
          ],
          [
           0.020425567403435707,
           0.02042577415704727,
           0.020429082214832306
          ],
          [
           0.02041579969227314,
           0.020416948944330215,
           0.02041882462799549
          ],
          [
           0.02040916495025158,
           0.020414598286151886,
           0.020415911450982094
          ],
          [
           0.02042124979197979,
           0.020420627668499947,
           0.020423782989382744
          ],
          [
           0.020420178771018982,
           0.020423471927642822,
           0.020423220470547676
          ],
          [
           0.020417803898453712,
           0.02041790448129177,
           0.02041899971663952
          ],
          [
           0.02040797285735607,
           0.02041112631559372,
           0.020410113036632538
          ],
          [
           0.020412245765328407,
           0.020411085337400436,
           0.02041390910744667
          ],
          [
           0.020388301461935043,
           0.020386969670653343,
           0.020389890298247337
          ],
          [
           0.02041769027709961,
           0.020416859537363052,
           0.020415883511304855
          ],
          [
           0.020404960960149765,
           0.02039909176528454,
           0.020405637100338936
          ],
          [
           0.020414987578988075,
           0.02041816897690296,
           0.020412033423781395
          ],
          [
           0.02042759582400322,
           0.020427383482456207,
           0.020426006987690926
          ],
          [
           0.020414777100086212,
           0.020412972196936607,
           0.02041536383330822
          ],
          [
           0.02041587419807911,
           0.02041417360305786,
           0.02040940709412098
          ],
          [
           0.020434778183698654,
           0.020439816638827324,
           0.020437508821487427
          ],
          [
           0.02040885202586651,
           0.020413750782608986,
           0.02041603811085224
          ],
          [
           0.02040005661547184,
           0.020400961861014366,
           0.020402733236551285
          ],
          [
           0.020409531891345978,
           0.020411521196365356,
           0.02041522227227688
          ],
          [
           0.020396266132593155,
           0.02039698325097561,
           0.02039598487317562
          ],
          [
           0.02040819078683853,
           0.02040962129831314,
           0.02041064389050007
          ],
          [
           0.020396409556269646,
           0.02039509080350399,
           0.020400673151016235
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"108b46e4-5d13-4c51-b2de-049ac7e88ed5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"108b46e4-5d13-4c51-b2de-049ac7e88ed5\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '108b46e4-5d13-4c51-b2de-049ac7e88ed5',\n",
       "                        [{\"colorscale\": \"Reds\", \"type\": \"heatmap\", \"uid\": \"2ede232b-7de2-4525-a92f-f3f09465af7a\", \"x\": [\"were\", \"not\", \"guilty\"], \"y\": [\"figlia\", \"pregevoli\", \"Alaska\", \"Liber\\u00f2\", \"Tiratevi\", \"straccio\", \"rilassarvi\", \"correggeteli\", \"stappate\", \"tradizionali\", \"arata\", \"sprofond\\u00f2\", \"Canzonai\", \"Compreremo\", \"Pitturavo\", \"astiene\", \"spiona\", \"guardato\", \"deludeteci\", \"lasciami\", \"allievi\", \"inventario\", \"posato\", \"Formarono\", \"suo\", \"guardino\", \"preparer\\u00f2\", \"didentit\\u00e0\", \"fornirebbe\", \"Moltissime\", \"collera\", \"risparmi\\u00f2\", \"culmine\", \"risposer\\u00f2\", \"dimentichiamocelo\", \"fotografie\", \"freccetta\", \"Rubare\", \"perfettina\", \"precauzione\", \"Avenue\", \"allent\\u00f2\", \"imparer\\u00e0\", \"piovuto\", \"Blanco\", \"assicurate\", \"enciclopedia\", \"caviale\", \"Ce\", \"comprarono\"], \"z\": [[0.020395765081048012, 0.020399510860443115, 0.020400796085596085], [0.020403439179062843, 0.02040637843310833, 0.020407628268003464], [0.020397108048200607, 0.020402805879712105, 0.02040446177124977], [0.02042507566511631, 0.020426997914910316, 0.02042900212109089], [0.020408837124705315, 0.020413434132933617, 0.020413484424352646], [0.02041882462799549, 0.020419759675860405, 0.020423099398612976], [0.020396852865815163, 0.020398659631609917, 0.020406097173690796], [0.020414678379893303, 0.02041710540652275, 0.020418493077158928], [0.02040308341383934, 0.02040311135351658, 0.02040119096636772], [0.02040223963558674, 0.020406009629368782, 0.02040562964975834], [0.020406804978847504, 0.02041100338101387, 0.020411742851138115], [0.020425690338015556, 0.02042669616639614, 0.02042372338473797], [0.020433271303772926, 0.02043539099395275, 0.020433610305190086], [0.02041829563677311, 0.0204179510474205, 0.020420249551534653], [0.020417682826519012, 0.02041599713265896, 0.02042311243712902], [0.020406324416399002, 0.02040700428187847, 0.02040827088057995], [0.020421138033270836, 0.020424284040927887, 0.020424045622348785], [0.02042362280189991, 0.020426683127880096, 0.020425401628017426], [0.020409535616636276, 0.020413266494870186, 0.02041909657418728], [0.020412810146808624, 0.02040928602218628, 0.020409876480698586], [0.020410796627402306, 0.020411141216754913, 0.02041318081319332], [0.02040826715528965, 0.02040942572057247, 0.02041500061750412], [0.0204131118953228, 0.020411044359207153, 0.02041759341955185], [0.020409949123859406, 0.020410429686307907, 0.02041080966591835], [0.020426271483302116, 0.02042290009558201, 0.020418046042323112], [0.020413096994161606, 0.02041323110461235, 0.020413847640156746], [0.020411981269717216, 0.02040986716747284, 0.020411545410752296], [0.02040516585111618, 0.020407147705554962, 0.020411627367138863], [0.020425567403435707, 0.02042577415704727, 0.020429082214832306], [0.02041579969227314, 0.020416948944330215, 0.02041882462799549], [0.02040916495025158, 0.020414598286151886, 0.020415911450982094], [0.02042124979197979, 0.020420627668499947, 0.020423782989382744], [0.020420178771018982, 0.020423471927642822, 0.020423220470547676], [0.020417803898453712, 0.02041790448129177, 0.02041899971663952], [0.02040797285735607, 0.02041112631559372, 0.020410113036632538], [0.020412245765328407, 0.020411085337400436, 0.02041390910744667], [0.020388301461935043, 0.020386969670653343, 0.020389890298247337], [0.02041769027709961, 0.020416859537363052, 0.020415883511304855], [0.020404960960149765, 0.02039909176528454, 0.020405637100338936], [0.020414987578988075, 0.02041816897690296, 0.020412033423781395], [0.02042759582400322, 0.020427383482456207, 0.020426006987690926], [0.020414777100086212, 0.020412972196936607, 0.02041536383330822], [0.02041587419807911, 0.02041417360305786, 0.02040940709412098], [0.020434778183698654, 0.020439816638827324, 0.020437508821487427], [0.02040885202586651, 0.020413750782608986, 0.02041603811085224], [0.02040005661547184, 0.020400961861014366, 0.020402733236551285], [0.020409531891345978, 0.020411521196365356, 0.02041522227227688], [0.020396266132593155, 0.02039698325097561, 0.02039598487317562], [0.02040819078683853, 0.02040962129831314, 0.02041064389050007], [0.020396409556269646, 0.02039509080350399, 0.020400673151016235]]}],\n",
       "                        {},\n",
       "                        {\"linkText\": \"Export to plot.ly\", \"responsive\": true, \"plotlyServerURL\": \"https://plot.ly\", \"showLink\": false}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('108b46e4-5d13-4c51-b2de-049ac7e88ed5');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bleu score on the text corpus is 0.0\n"
     ]
    }
   ],
   "source": [
    "predict_random_val_sentence(1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_random_val_sentence(len(input_tensor_test),False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
